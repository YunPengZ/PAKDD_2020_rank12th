{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import copy \n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from datetime import datetime\n",
    "# import seaborn as sns\n",
    "pd.set_option('display.max_columns', 100)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/round1_testB/disk_sample_smart_log_test_b.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = test.sort_values('dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_df(file_dir):\n",
    "    chunk_size = 1e6\n",
    "    df = pd.read_csv(file_dir,iterator = True)\n",
    "    res_df = []\n",
    "    while True:\n",
    "        try:\n",
    "            t = df.get_chunk(chunk_size)\n",
    "            na_t = t.isna().all()\n",
    "            inna_t = na_t[na_t==False]\n",
    "            res = t[inna_t.index]\n",
    "            res_df.append(res)\n",
    "        except:\n",
    "            print('once end')\n",
    "            break\n",
    "    df = pd.concat(res_df,ignore_index = True)\n",
    "    del res_df\n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################先看是否有label \n",
    "def get_label(df,fault):\n",
    "    t = pd.merge(df,fault,on=['serial_number','model'],how='left')\n",
    "    t['tag'] = t['tag'].fillna(-1).astype(int)\n",
    "    dtime = pd.to_datetime(t['dt'],format='%Y%m%d')\n",
    "    fault_time= pd.to_datetime(t['fault_time'],format='%Y-%m-%d')\n",
    "    faut_time_delta_days = (fault_time-dtime).dt.days\n",
    "    t['get_fault_in_30_days'] = np.where(faut_time_delta_days<=30,1,0)#需要大于0 么 如果是-1 即\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fault = pd.read_csv('../data/disk_sample_fault_tag.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_1807 = get_df('../data/round1_train/disk_sample_smart_log_201807.csv')\n",
    "df_1805 = get_df('../data/round1_train/disk_sample_smart_log_201805.csv')\n",
    "df_1806 = get_df('../data/round1_train/disk_sample_smart_log_201806.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['dt'] = pd.to_datetime(test['dt'],format='%Y%m%d')\n",
    "df_1807['dt'] = pd.to_datetime(df_1807['dt'],format='%Y%m%d')\n",
    "df_1806['dt'] = pd.to_datetime(df_1806['dt'],format='%Y%m%d')\n",
    "df_1805['dt'] = pd.to_datetime(df_1805['dt'],format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_train = pd.concat([df_1805,df_1806])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drop_cols = ['smart_3raw','smart_10raw','smart_240_normalized','smart_242_normalized','smart_241_normalized']\n",
    "test_train = test_train.drop(drop_cols,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_train = get_label(test_train,fault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_unique_id(df):\n",
    "    df['unique_id'] = df['serial_number'].map(str)+'_'+df['model'].map(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_diff_rate(df,feature_aug):\n",
    "    for fea in feature_aug:\n",
    "        print(fea)\n",
    "        df[fea+'_diff_shift'] = df.groupby(['serial_number','model'])[fea+'_diff'].shift(1,axis=0)\n",
    "        df[fea+'_diff_rate'] = df[fea+'_diff']/df[fea+'_diff_shift']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def agg_features(df,col,agg_params,suffix=''):\n",
    "    agg_dict = {}\n",
    "    for agg in agg_params:\n",
    "        agg_dict[col+'_'+agg+suffix] = agg\n",
    "    t = df.groupby(['serial_number','model'])[col].agg(agg_dict).reset_index()\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_kind_feature(old,cur_date,res,feature,agg_params,absolute=False):#获得每一类特征的一天，三天，一个月内的数据\n",
    "    old_recent_1_days = old[old.dt==cur_date-pd.Timedelta(days=1)]\n",
    "    old_recent_3_days = old[old.dt>=cur_date- pd.Timedelta(days=3)]\n",
    "    old_recent_5_days = old[old.dt>=cur_date- pd.Timedelta(days=5)]\n",
    "    old_recent_7_days = old[old.dt>=cur_date- pd.Timedelta(days=7)]\n",
    "    old_recent_15_days = old[old.dt>=cur_date- pd.Timedelta(days=15)]\n",
    "#     old_recent_month = old[old.dt>=cur_date- pd.Timedelta(days=30)]\n",
    "###############################################################################平均值删掉\n",
    "    if absolute:\n",
    "        res[feature] = np.abs(res[feature])\n",
    "    t = agg_features(old,feature,agg_params)\n",
    "    res = pd.merge(res,t,on=['serial_number','model'],how='left')#res是group by 后的值\n",
    "    t = old_recent_1_days.groupby(['serial_number','model'])[feature].agg({feature+'_1_days':'mean'})#可能会有多值\n",
    "    res = pd.merge(res,t,on=['serial_number','model'],how='left')\n",
    "    ###################################################3333\n",
    "    t = agg_features(old_recent_3_days,feature,agg_params,'_3_days')\n",
    "    res = pd.merge(res,t,on=['serial_number','model'],how='left')\n",
    "    t = agg_features(old_recent_5_days,feature,agg_params,'_5_days')    \n",
    "    res = pd.merge(res,t,on=['serial_number','model'],how='left')\n",
    "        ##########################################7\n",
    "    t = agg_features(old_recent_7_days,feature,agg_params,'_7_days')    \n",
    "    res = pd.merge(res,t,on=['serial_number','model'],how='left')\n",
    "    t = agg_features(old_recent_15_days,feature,agg_params,'_15_days')\n",
    "    res = pd.merge(res,t,on=['serial_number','model'],how='left')\n",
    "#     t = agg_features(old_recent_month,feature,agg_params,'_recent_month')\n",
    "#     res = pd.merge(res,t,on=['serial_number','model'],how='left')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_diff_rate(df,col,absolute=False):\n",
    "    df[col+'_1_days_diff'] = df[col]-df[col+'_1_days']\n",
    "    df[col+'_3_days_diff'] = df[col]-df[col+'_mean_3_days']\n",
    "    df[col+'_5_days_diff'] = df[col]-df[col+'_mean_5_days']\n",
    "    df[col+'_7_days_diff'] = df[col]-df[col+'_mean_7_days']\n",
    "    df[col+'_15_days_diff'] = df[col]-df[col+'_mean_15_days']\n",
    "    df[col+'_recent_month_diff'] = df[col]-df[col+'_mean']\n",
    "    if absolute==True:\n",
    "        for suffix in ['_1_days_diff','_5_days_diff','_15_days_diff','_recent_month_diff']:\n",
    "            df[col+suffix] = np.abs(df[col+suffix])\n",
    "#     df[col+'_one_day_diff_rate'] = df[col+'_1_days_diff']/df[col+'_1_days']  #相较于上一天\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#构造30天的训练集 和验证集\n",
    "def construct_train(old,cur,cur_date,test=False):\n",
    "    drop_features = ['dt','fault_time','tag','manufacturer','unique_id','diff_day','get_fault_in_30_days','manufacturer_x','manufacturer_y']\n",
    "    keep_features = [fea for fea in test_train.columns if fea not in drop_features+drop_cols]\n",
    "    if test==False:\n",
    "        keep_features.append('get_fault_in_30_days')\n",
    "    res = pd.DataFrame(cur[keep_features])#现有的数据  应该是现有数据和历史数据结合 现有数据全部保留 所以这样写 问题不大\n",
    "    #####################################\n",
    "    #统计历史数据 全部历史的统计量\n",
    "    ############### smart_1 raw\n",
    "    res = get_kind_feature(old,cur_date,res,'smart_1_normalized',['mean'])\n",
    "    ############### smart_1 raw\n",
    "    res = get_kind_feature(old,cur_date,res,'smart_1raw',['mean'])\n",
    "    #################smart 3 normalized\n",
    "    res = get_kind_feature(old,cur_date,res,'smart_3_normalized',['max','mean'])\n",
    "    #################smart 4 normalized\n",
    "#     res = get_kind_feature(old,cur_date,res,'smart_4_normalized',['max'])\n",
    "    #################smart 4 normalized\n",
    "#     res = get_kind_feature(old,cur_date,res,'smart_4raw',['max'])\n",
    "    #####################################smart 5\n",
    "    #5是一直在增长的 转化为normalize之后应该是一直在下降的 但是只要下降趋势是稳定的就可以 下降趋势\n",
    "    #下降趋势应该怎么表示呢 30天变化值 除以3天的平均值 \n",
    "    res = get_kind_feature(old,cur_date,res,'smart_5_normalized',['mean'])\n",
    "    res = get_kind_feature(old,cur_date,res,'smart_5raw',['mean'])\n",
    "    ################### smart_7\n",
    "    res = get_kind_feature(old,cur_date,res,'smart_7raw',['mean'])\n",
    "    res = get_kind_feature(old,cur_date,res,'smart_7_normalized',['mean'])\n",
    "    ################### smart_10\n",
    "    res = get_kind_feature(old,cur_date,res,'smart_10_normalized',['mean'])\n",
    "    ################### smart_184 出厂时的坏盘记录 这个应该对磁盘的不同日期取值都一样吧\n",
    "    ### 187 数据不为零（突然增大 则为异常点）\n",
    "    res = get_kind_feature(old,cur_date,res,'smart_187raw',['mean'])\n",
    "    ################### smart_188\n",
    "#     res = get_kind_feature(old,cur_date,res,'smart_188raw',['max'])\n",
    "#     res = get_kind_feature(old,cur_date,res,'smart_188_normalized',['max'])\n",
    "     ################### smart_191\n",
    "#     res = get_kind_feature(old,cur_date,res,'smart_191raw',['mean','max'])\n",
    "    ################### smart_193\n",
    "    res = get_kind_feature(old,cur_date,res,'smart_193raw',['mean'])\n",
    "    ################### smart_192\n",
    "    res = get_kind_feature(old,cur_date,res,'smart_192_normalized',['mean'])\n",
    "    ################### smart_194\n",
    "    res = get_kind_feature(old,cur_date,res,'smart_194_normalized',['mean','max'])\n",
    "    ################### smart_195\n",
    "    res = get_kind_feature(old,cur_date,res,'smart_195_normalized',['mean','max'])\n",
    "    ################### smart_197\n",
    "    res = get_kind_feature(old,cur_date,res,'smart_197_normalized',['mean','max'])\n",
    "    res = get_kind_feature(old,cur_date,res,'smart_197raw',['mean','max'])\n",
    "\n",
    "    ################### smart_198\n",
    "    res = get_kind_feature(old,cur_date,res,'smart_198_normalized',['mean'])\n",
    "    ################### smart_199\n",
    "    res = get_kind_feature(old,cur_date,res,'smart_199raw',['mean','max'])\n",
    "\n",
    "#     res = get_kind_feature(old,cur_date,res,'smart_199_normalized',['max'])\n",
    "    ################### smart_240\n",
    "    res = get_kind_feature(old,cur_date,res,'smart_240raw',['mean'])\n",
    "    ################### smart_242\n",
    "    res = get_kind_feature(old,cur_date,res,'smart_242raw',['mean'])\n",
    "    ####################################\n",
    "    #添加变化列和统计变化幅度\n",
    "    res = get_diff_rate(res,'smart_1_normalized')#变化幅度和变化幅度除以原始的值 变化的值在与一周内比较 必须使用平均值\n",
    "#     res = get_diff_rate(res,'smart_3_normalized')\n",
    "    res = get_diff_rate(res,'smart_5_normalized')\n",
    "    res = get_diff_rate(res,'smart_7raw')\n",
    "#     res = get_diff_rate(res,'smart_10_normalized')\n",
    "    res = get_diff_rate(res,'smart_187raw')\n",
    "    res = get_diff_rate(res,'smart_192_normalized')\n",
    "    res = get_diff_rate(res,'smart_194_normalized')\n",
    "    res = get_diff_rate(res,'smart_195_normalized')\n",
    "    res = get_diff_rate(res,'smart_197_normalized')\n",
    "    #变化量除以原始的均值 表示变换量\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def merge_history(df,date_range,isTest=False):#对历史数据的处理 比如对5月2号 历史数据为5月1号数据加上之前的数据\n",
    "    df_list = []\n",
    "    for index,cur_date in enumerate(date_range):\n",
    "        #31开始是不用的文件\n",
    "        index = index + 31\n",
    "        print('day',cur_date,index)\n",
    "        start = time.time()\n",
    "        #取五月的数据做训练\n",
    "        #############################\n",
    "        #历史数据 \n",
    "        old = df[df.dt<cur_date][df.dt>=cur_date- pd.Timedelta(days=30)]\n",
    "        ###############################\n",
    "        #取当天的成绩\n",
    "        cur = df[df.dt==cur_date]\n",
    "        ##############################\n",
    "        #两者合并\n",
    "        print('before merged shape',old.shape)\n",
    "        construct_df = construct_train(old,cur,cur_date,test=isTest)\n",
    "        construct_df['dt'] =cur_date\n",
    "        print('after merged shape',construct_df.shape)\n",
    "#         construct_df.to_pickle('./comb/val_v2_%02d.pkl'% index)\n",
    "        end = time.time()\n",
    "        print(end-start)\n",
    "        df_list.append(construct_df)\n",
    "    history = pd.concat(df_list).reset_index(drop=True)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_train = merge_history(test_train,pd.date_range(start='20180601',end='20180630'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_train.to_csv('../user_data/tmp_data/train_data/test_06.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = get_unique_id(test)\n",
    "test_ids = test['unique_id'].unique()\n",
    "df_1807 = get_unique_id(df_1807)\n",
    "df_1807_test = df_1807[df_1807.unique_id.isin(test_ids)]\n",
    "test_df = pd.concat([df_1807_test,test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day 2018-08-01 00:00:00 31\n",
      "before merged shape (297463, 516)\n",
      "after merged shape (10649, 405)\n",
      "39.78970217704773\n",
      "day 2018-08-02 00:00:00 32\n",
      "before merged shape (298242, 516)\n",
      "after merged shape (10277, 405)\n",
      "37.78570771217346\n",
      "day 2018-08-03 00:00:00 33\n",
      "before merged shape (298614, 516)\n",
      "after merged shape (9935, 405)\n",
      "37.7097008228302\n",
      "day 2018-08-04 00:00:00 34\n",
      "before merged shape (298671, 516)\n",
      "after merged shape (9547, 405)\n",
      "35.871989727020264\n",
      "day 2018-08-05 00:00:00 35\n",
      "before merged shape (298182, 516)\n",
      "after merged shape (9198, 405)\n",
      "35.05901789665222\n",
      "day 2018-08-06 00:00:00 36\n",
      "before merged shape (297289, 516)\n",
      "after merged shape (8837, 405)\n",
      "33.44669032096863\n",
      "day 2018-08-07 00:00:00 37\n",
      "before merged shape (295968, 516)\n",
      "after merged shape (8444, 405)\n",
      "32.8575918674469\n",
      "day 2018-08-08 00:00:00 38\n",
      "before merged shape (294265, 516)\n",
      "after merged shape (7752, 405)\n",
      "30.7946834564209\n",
      "day 2018-08-09 00:00:00 39\n",
      "before merged shape (291875, 516)\n",
      "after merged shape (7539, 405)\n",
      "30.419849157333374\n",
      "day 2018-08-10 00:00:00 40\n",
      "before merged shape (289248, 516)\n",
      "after merged shape (7928, 405)\n",
      "29.613616943359375\n",
      "day 2018-08-11 00:00:00 41\n",
      "before merged shape (287043, 516)\n",
      "after merged shape (7569, 405)\n",
      "29.293720483779907\n",
      "day 2018-08-12 00:00:00 42\n",
      "before merged shape (284437, 516)\n",
      "after merged shape (7218, 405)\n",
      "28.87866473197937\n",
      "day 2018-08-13 00:00:00 43\n",
      "before merged shape (281524, 516)\n",
      "after merged shape (6846, 405)\n",
      "28.906293153762817\n",
      "day 2018-08-14 00:00:00 44\n",
      "before merged shape (278235, 516)\n",
      "after merged shape (6480, 405)\n",
      "28.096303701400757\n",
      "day 2018-08-15 00:00:00 45\n",
      "before merged shape (274551, 516)\n",
      "after merged shape (6090, 405)\n",
      "26.68891716003418\n",
      "day 2018-08-16 00:00:00 46\n",
      "before merged shape (270677, 516)\n",
      "after merged shape (5749, 405)\n",
      "24.545485734939575\n",
      "day 2018-08-17 00:00:00 47\n",
      "before merged shape (266300, 516)\n",
      "after merged shape (5180, 405)\n",
      "23.89464235305786\n",
      "day 2018-08-18 00:00:00 48\n",
      "before merged shape (261381, 516)\n",
      "after merged shape (4843, 405)\n",
      "22.979464530944824\n",
      "day 2018-08-19 00:00:00 49\n",
      "before merged shape (256096, 516)\n",
      "after merged shape (4484, 405)\n",
      "21.900243043899536\n",
      "day 2018-08-20 00:00:00 50\n",
      "before merged shape (250363, 516)\n",
      "after merged shape (4110, 405)\n",
      "21.198346614837646\n",
      "day 2018-08-21 00:00:00 51\n",
      "before merged shape (244216, 516)\n",
      "after merged shape (3726, 405)\n",
      "20.334843397140503\n",
      "day 2018-08-22 00:00:00 52\n",
      "before merged shape (237698, 516)\n",
      "after merged shape (3409, 405)\n",
      "19.266295194625854\n",
      "day 2018-08-23 00:00:00 53\n",
      "before merged shape (230822, 516)\n",
      "after merged shape (2871, 405)\n",
      "18.43877935409546\n",
      "day 2018-08-24 00:00:00 54\n",
      "before merged shape (227310, 516)\n",
      "after merged shape (2701, 405)\n",
      "17.608656644821167\n",
      "day 2018-08-25 00:00:00 55\n",
      "before merged shape (219626, 516)\n",
      "after merged shape (2326, 405)\n",
      "17.079652309417725\n",
      "day 2018-08-26 00:00:00 56\n",
      "before merged shape (211484, 516)\n",
      "after merged shape (1973, 405)\n",
      "15.92690396308899\n",
      "day 2018-08-27 00:00:00 57\n",
      "before merged shape (202930, 516)\n",
      "after merged shape (1601, 405)\n",
      "14.559434652328491\n",
      "day 2018-08-28 00:00:00 58\n",
      "before merged shape (195613, 516)\n",
      "after merged shape (1387, 405)\n",
      "13.95137643814087\n",
      "day 2018-08-29 00:00:00 59\n",
      "before merged shape (188149, 516)\n",
      "after merged shape (1018, 405)\n",
      "12.771734714508057\n",
      "day 2018-08-30 00:00:00 60\n",
      "before merged shape (180269, 516)\n",
      "after merged shape (654, 405)\n",
      "12.501280546188354\n",
      "day 2018-08-31 00:00:00 61\n",
      "before merged shape (170341, 516)\n",
      "after merged shape (309, 405)\n",
      "10.926231145858765\n"
     ]
    }
   ],
   "source": [
    "res_df = merge_history(test_df,pd.date_range(start='20180801',end='20180831'),True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_df.to_csv('../user_data/tmp_data/test_data/test_v3.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
